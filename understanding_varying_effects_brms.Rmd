---
title: "Understanding Varying Effects with brms"
author: "Anders Sundelin"
date: "2022-12-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(ggcorrplot)
library(tidyr)
library(dplyr)
library(brms)
library(dagitty)
library(stringr)
library(rethinking)
library(bayesplot)
library(tidybayes)
library(distributional)
```

## Data Ingestion and Model Building

On my laptop, it takes about 10 minutes per model to run this Rmd file. Beware!

```{r ingest}
df <- read.csv("samples/authors-team-impact.csv")

scale_cloc      <- data.frame(t(c(count=1771, mean=123.6285, stddev=303.9526, median=56)))
scale_mccabe    <- data.frame(t(c(count=1771, mean=13.70073, stddev=40.93051, median=5)))
scale_duplines  <- data.frame(t(c(count=1771, mean=18.04743, stddev=59.00211, median=0)))
scale_dupblocks <- data.frame(t(c(count=1771, mean=0.9994353, stddev=3.071005, median=0)))
scale_added <- df %>% summarize(count=n(), mean=mean(added), stddev=sd(added), median=median(added))
scale_removed <- df %>% summarize(count=n(), mean=mean(removed), stddev=sd(removed), median=median(removed))

data_centered <- df %>% mutate(file=fileid, author=authorid, team=authorteamid,
                               ISTEST=istestfile,
                               ADD=(added-scale_added$mean)/scale_added$stddev,
                               DEL=(removed-scale_removed$mean)/scale_removed$stddev,
                               CLOC=(currCloc-scale_cloc$mean)/scale_cloc$stddev,
                               COMPLEX=(currComplex-scale_mccabe$mean)/scale_mccabe$stddev,
                               DUP=(prevDupBlocks-scale_dupblocks$mean)/scale_dupblocks$stddev,
                               INTROD=if_else(delta >= 0, delta, as.integer(0)),
                               REMOVED=if_else(delta <= 0, delta, as.integer(0)),
                               y=INTROD) %>%
  select(file, author, team, ADD, DUP, y)

data_scaled <- df %>% mutate(file=fileid, author=authorid, team=authorteamid,
                             ISTEST=istestfile,
                             ADD=(added)/scale_added$stddev,
                             DEL=(removed)/scale_removed$stddev,
                             CLOC=(currCloc)/scale_cloc$stddev,
                             COMPLEX=(currComplex)/scale_mccabe$stddev,
                             DUP=(prevDupBlocks)/scale_dupblocks$stddev,
                             INTROD=if_else(delta >= 0, delta, as.integer(0)),
                             REMOVED=if_else(delta <= 0, delta, as.integer(0)),
                             y=INTROD) %>%
  select(file, author, team, ADD, DUP, y)
data <- data_scaled

data %>% summarize(count = n(), mean(y), sd(y))

data %>% filter(y > 0) %>% summarize(count = n(), mean(y), sd(y))
```

## Prior Predictive Checks

Getting brms to tell us what priors it expects when it comes to our model boils down to using the `get_prior` function properly, understanding its output.

```{r}
get_prior(data=data,
          family = zero_inflated_negbinomial,
          formula = bf(y ~ 0 + (1 + author | team) + ADD + DUP, zi ~ 1 + ADD + DUP))
```

If I want to use `sample_prior = "only"`, in order to only let the priors influence the result, I also need to specify a specific prior for the zero-inflation `dpar` part.

```{r}
pp_model <- brm(data = data,
      family = zero_inflated_negbinomial,
      bf(y ~ 0 + (1 + author | team) + ADD + DUP,
         zi ~ 1 + ADD + DUP),
      prior = c(prior(normal(0, 0.2), class = b),
                prior(normal(0, 0.2), class = b, dpar=zi),
                prior(weibull(2, 1), class = sd),
                prior(lkj(2), class = cor),
                prior(gamma(0.01, 0.01), class = shape)),
      sample_prior = "only",
      warmup = 1000,
      iter  = 4000,
      chains = 4,
      cores = 4,
      backend="cmdstanr",
      threads = threading(2),
      adapt_delta = 0.95)
```

I'd say 6% divergent transitions show that we have some wild priors.
Setting a stddev for the b-par of 0.2 instead of 0.25 decrease it to 4% divergent transitions.



```{r}
pp_check(pp_model)
```

```{r}
pp_check(pp_model, type="error_hist", ndraws=11)
```

loo_pit will not work, NAs is produced.

```{r}
pp_check(pp_model, type="scatter_avg", ndraws=100)
```

We have some y values on the y axes that are reasonable, and some $y_{rep}$ values on the x axis that are totally wild.



### Null model
```{r}
# m0 <-
#   brm(data = data,
#       family = zero_inflated_negbinomial,
#       y ~ 1,
#       chains = 4, cores = 4, threads = threading(2))
#
# Our proposed model where I only sample from priors
# I've only done very rudimentary PriPC ad hoc in terminal
# m_prior <-
#   brm(data = data,
#       family = zero_inflated_negbinomial,
#       y ~ 1 + author + (1 + author | team),
#       prior = c(prior(normal(0, 2), class = Intercept),
#                 prior(normal(0, 0.5), class = b),
#                 prior(exponential(1), class = sd),
#                 prior(lkj(2), class = cor)),
#       chains = 4, cores = 4, threads = threading(2), sample_prior = "only")
```

### Team Intercept, Author Slope

We ignore this group-level model, where the teams groups the data, controllin the intercept, and the author controls the modification to that intercept.

I still need to figure out what the significance of the population-level author is, and the author in the group.

Basically, a group-level parameter can also occur on the population level - how should I think about this?

```{r, eval=FALSE}
set.seed(700716)
m_nb <-
  brm(data = data,
      family = zero_inflated_negbinomial,
      y ~ 0 + author + (1 + author | team),
      prior = c(prior(normal(0, 0.5), class = b),
                prior(weibull(2, 1), class = sd),
                prior(lkj(2), class = cor),
                prior(beta(1, 1), class = zi),
                prior(gamma(0.01, 0.01), class = shape)),
      warmup = 1000,
      iter  = 2000,
      chains = 4,
      backend="cmdstanr",
      threads = threading(2),
      cores = 4)

```
### Team Intercept, Author Slope, Additions and Existing Duplicates

This model assumes a fixed zero-inflation probability for all files and changes.
In the summary output, it is stated:

    Links: mu = log; shape = identity; *zi = identity*


```{r}
set.seed(700716)
m_nb_add_dup <-
  brm(data = data,
      family = zero_inflated_negbinomial,
      y ~ 0 + author + (1 + author | team) + ADD + DUP,
      prior = c(prior(normal(0, 0.25), class = b),
                prior(weibull(2, 1), class = sd),
                prior(lkj(2), class = cor),
                prior(beta(1, 1), class = zi),
                prior(gamma(0.01, 0.01), class = shape)),
      warmup = 1000,
      iter  = 2000,
      chains = 4,
      cores = 4,
      backend="cmdstanr",
      threads = threading(2),
      adapt_delta = 0.95)
```

```{r}
summary(m_nb_add_dup)
```

We had one divergent transition, but we will move on to a more reasonable model, as the zero-inflation is highly influenced by additions and existing duplicates.

### Controlling the zero-inflation via parameters

We use the `bf` function to build a composite formula, where the zero-inflation part also can be controlled by parameters from the data.

The `get_priors` function is used to get brms to show what kinds of priors that are possible to set, for any given formula.
Note that brms, uses a different philosophy than `rethinking` --- it will use default priors, unless you set them to some function, and you can set priors based on the class of the parameter being estimated.

Classes include:

* `b` --- a normal beta (slope parameter or intercept)
* `sd` --- a standard-deviation parameter (always non-negative)
* `cor` --- a correlation matrix, for instance between intercepts and slopes (where we typically use an lkj)
* `shape` --- a shape parameter to distributions such as the Gamma distribution

We also remove the population-level author effect, leaving the author effect only inside the team (which controls the intercept).

Zero-inflation is controlled only by the added lines and existing duplicates in the file (plus the base intercept of course).

```{r}
set.seed(700716)
m_zi_nb_add_dup <-
  brm(data = data,
      family = zero_inflated_negbinomial,
      bf(y ~ 0 + (1 + author | team) + ADD + DUP,
         zi ~ 1 + ADD + DUP),
      prior = c(prior(normal(0, 0.25), class = b),
                prior(weibull(2, 1), class = sd),
                prior(lkj(2), class = cor),
                prior(gamma(0.01, 0.01), class = shape)),
      warmup = 1000,
      iter  = 4000,
      chains = 4,
      cores = 4,
      backend="cmdstanr",
      threads = threading(2),
      adapt_delta = 0.95)
```



```{r}
get_prior(formula = bf(y ~ 0 + author + (1 + author | team) + ADD + DUP, zi ~ 1 + ADD + DUP),
          data=data,
          family=zero_inflated_negbinomial)
```

```{r}
summary(m_zi_nb_add_dup)
```


## Model Comparison


```{r, eval=FALSE}
m_nb <- add_criterion(m_nb, criterion = "loo")
m_zi_nb_add_dup <- add_criterion(m_zi_nb_add_dup, criterion = "loo")
loo_compare(m_nb, m_nb_add_dup)
```

Adding ADD and DUP to our model have great effect.


```{r}
m_nb_add_dup <- add_criterion(m_nb_add_dup, criterion = "loo")
m_zi_nb_add_dup <- add_criterion(m_zi_nb_add_dup, criterion = "loo")
loo_compare(m_zi_nb_add_dup, m_nb_add_dup)

```

As is expected, the model where zi was dependent on the ADD and DUP parameters performed better.



We select the model with ADD and DUP for both the rate and the zero-inflation and use it for illustrating the data.

```{r}
M <- m_zi_nb_add_dup
M
```

### Diagnostics

```{r}
neff_ratio(M)
```

Manually running mcmc_trace in the plot area shows more details.

```{r}
stopifnot(rhat(M) < 1.01)
stopifnot(neff_ratio(M) > 0.20)
mcmc_trace(M) # ok
```

Checking R-hat and number of effective parameters.

The trace plots of the MCMC walk should converge and reach some sort of "mixed state". Divergence indicates an ill-fitted model.

```{r}
np <- nuts_params(M)
lp <- log_posterior(M)
mcmc_nuts_divergence(np, lp) # ok
```



```{r}
loo_M <- loo(M, save_psis = TRUE) # ok
print(loo_M)
```


### Posterior

```{r}
# check how the posterior for the correlation of the random effects
# compares to prior
post <- as_draws_df(M)

```

```{r}
r_2 <-
  rlkjcorr(1e4, K = 2, eta = 2) |>
  data.frame()

post %>%
      ggplot() +
      geom_density(data = r_2, aes(x = X2),
                   color = "transparent", fill = "blue", alpha = 3/4) +
      geom_density(aes(x = cor_team__Intercept__author),
                   color = "transparent", fill = "#A65141", alpha = 9/10) +
      annotate(geom = "text",
               x = c(-0.6, 0), y = c(2.2, 1.0),
               label = c("posterior", "prior"),
               color = c("#A65141", "blue")) +
      scale_y_continuous(NULL, breaks = NULL) +
      labs(subtitle = "Correlation between intercepts and slopes, prior and posterior",
      x = "correlation")
# we have moved towards slight positive correlation, around 0.4-0.6 or so
```

### Understanding Posterior Shape

```{r}
mcmc_areas_ridges(M, regex_pars = "sd_team__")
```

```{r}
mcmc_areas_ridges(M, pars = c("r_team[1,author]","r_team[2,author]","r_team[3,author]", "r_team[4,author]","r_team[5,author]","r_team[6,author]","r_team[7,author]","r_team[8,author]","r_team[9,author]","r_team[10,author]","r_team[11,author]"), prob = 0.95)
```

How shall I understand these parameters?


```{r}
mcmc_areas_ridges(M, pars = c("r_team[1,Intercept]","r_team[2,Intercept]","r_team[3,Intercept]","r_team[4,Intercept]","r_team[5,Intercept]","r_team[6,Intercept]","r_team[7,Intercept]","r_team[8,Intercept]","r_team[9,Intercept]","r_team[10,Intercept]",
    "r_team[11,Intercept]"), prob = 0.95)
```

We see some teams that have very wide distributions for the intercept. But many teams have firmly negative intercepts.

```{r}
mcmc_areas_ridges(M, regex_pars = "^b_")
```

The Zero-Inflation parameters dominate. For a small change (ADD is close to 0), the highly positive zi-intercept will make it unlikely to be any duplicates added.
But, for bigger changes (ADD close to 1 stddev) will cancel out the intercept, and then it will be the number of duplicates that control whether there is any issues added.

```{r, eval=FALSE}
pp_check(M, type = "error_hist", ndraws = 11)
```
I really have to understand these checks.


```{r}
pp_check(M, type = "loo_pit")

```


## Posterior predictions

Following Andrew Heiss, https://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/

Let's predict what the model says about a new change that comes in.
If we let author 5 in team 7 add 30 lines to file 17, which had 6 existing duplicated blocks, how likely is s/he to introduce duplicates?


### Posterior predictions of the linear parameter

posterior_linpred uses draws of the linear predictor (before any link function has been applied).

We plot the value of an imaginary change to file 17, by author 5, belonging to team 7, comprising 30 changed lines, and the file has 6 existing duplicates.

```{r}
newdata <- data.frame(t(c(file=17, author=5, team=7, ADD=30/scale_added$stddev, DUP=6/scale_dupblocks$stddev)))

issues_linpred <- M |> linpred_draws(newdata=newdata)
```


So, this is on the logit scale - the linear part, before linking.

```{r}
issues_linpred |> ggplot(aes(x=.linpred)) + stat_halfeye() + scale_x_continuous()
```

Question - as we are dealing with a multi-model, is this linear prediction related to the zero-inflation part, or the lambda (rate & shape) part?
The value of -1 seems to indicate that zero-inflation plays a part

### Posterior predictions of the expected parameter value

epred draws returns the expected value of the parameter (what parameter?)

```{r}
issues_epred <- M |> epred_draws(newdata=newdata)
issues_epred |> ggplot(aes(x=.epred)) + stat_halfeye() + scale_x_continuous()

```

### Posterior predictions on the outcome (y) scale

And in the predicted_draws we have the probabilities on the outcome scale.
We see the high probability of zero issues being introduced, with correspondingly lesser probability of introducing 1, 2, 3,... issues.

```{r}
issues_predicted <- M |> predicted_draws(newdata=newdata)
issues_predicted |> ggplot(aes(x=.prediction)) + stat_halfeye() + scale_x_continuous()

```

### Author effects

If we look at the author who introduced the most issues, in absolute and relative terms.

```{r}
data %>% group_by(author) %>% summarize(issues = sum(y)) %>% arrange(desc(issues)) %>% head(5)
```

```{r}
data %>% filter(y > 0) %>% group_by(author) %>% summarize(relissues = sum(y)/n()) %>% arrange(desc(relissues)) %>% head(5)
```

So, we see that author 10 introduced the most issues overall (121), but relative to the number of issues introduced per file (that had issues introduced), the highest number of issues came from author 68.


Finding authors that introduce few issues are harder. We have to look at all the changes (also those files with y == 0), and see the relative weight of the number of introduced issues per file.

```{r}
data %>% group_by(author) %>% summarize(relissues = sum(y)/n(), sum(y), count=n()) %>% arrange(desc(relissues)) %>% tail(30)

```

Author 17 and 6 never introduced any issues, yet changed 411 and 393 files, respectively. They could be considered "good examples".


How do these compare?

Base, random author, random team:

```{r}
newdata <- data.frame(t(c(file=17, author=5, team=7, ADD=30/scale_added$stddev, DUP=6/scale_dupblocks$stddev)))
M |> predicted_draws(newdata=newdata) |> ggplot(aes(x=.prediction)) + stat_halfeye() + scale_x_continuous() + coord_cartesian(ylim=c(0,0.1))
```




```{r}
newdata <- data.frame(t(c(file=17, author=10, team=7, ADD=30/scale_added$stddev, DUP=6/scale_dupblocks$stddev)))
predicted_author10 <- M |> predicted_draws(newdata=newdata)
predicted_author10 |> ggplot(aes(x=.prediction)) + stat_halfeye() + scale_x_continuous() + coord_cartesian(ylim=c(0,0.1))

```

```{r}
newdata <- data.frame(t(c(file=17, author=68, team=7, ADD=30/scale_added$stddev, DUP=6/scale_dupblocks$stddev)))
predicted_author68 <- M |> predicted_draws(newdata=newdata)
predicted_author68 |> ggplot(aes(x=.prediction)) + stat_halfeye() + scale_x_continuous() + coord_cartesian(ylim=c(0,0.1))

```

So, we see that author 68 actually have lower maximum a posteriori probability (peaking around 0.06) to introduce one issue in a file as we specified.
Whereas author 10 has about 8% maximum a posteriori probability to introduce one issue. But note the scale on the x axis.
Author 10 has a tendency to make smaller additions of issues. Author 68 has a wider span (but this could also be due to chances - we really need to look att the stddev of the parameter)

```{r}
max(predicted_author10$.prediction)
max(predicted_author68$.prediction)
```
### Team effects

```{r}
data %>% group_by(team) %>% summarize(issues = sum(y)) %>% arrange(desc(issues))
```

```{r}
data %>% filter(y > 0) %>% group_by(team) %>% summarize(relissues = sum(y)/n()) %>% arrange(desc(relissues))
```

Team 2 seems to be introducing most issues in this repository, both in absolute and relative terms.

Which team has made most changes overall?

```{r}
data %>% group_by(team) %>% summarize(fileschanged = n()) %>% arrange(desc(fileschanged))

```

Team 6 have changed about a third of the files of the maximum team, but are still the top 3 team for introducing duplicates.
How do our authors fare if we put them in that team?

```{r}
newdata <- data.frame(t(c(file=17, author=10, team=6, ADD=30/scale_added$stddev, DUP=6/scale_dupblocks$stddev)))
predicted_author10_team6 <- M |> predicted_draws(newdata=newdata)
predicted_author10_team6 |> ggplot(aes(x=.prediction)) + stat_halfeye() + scale_x_continuous() + coord_cartesian(ylim=c(0,0.1))

```

Much more probability between the spikes!

```{r}
newdata <- data.frame(t(c(file=17, author=68, team=6, ADD=30/scale_added$stddev, DUP=6/scale_dupblocks$stddev)))
predicted_author68_team6 <- M |> predicted_draws(newdata=newdata)
predicted_author68_team6 |> ggplot(aes(x=.prediction)) + stat_halfeye() + scale_x_continuous() + coord_cartesian(ylim=c(0,0.1))

```

Putting author 68 into team 2, doing a larger change of 300 lines, in a file with 6 existing duplicates mean that there would be around 18% probability of introducing a duplicate, and around 3% of introducing 5.

```{r}
newdata <- data.frame(t(c(file=17, author=68, team=2, ADD=300/scale_added$stddev, DUP=6/scale_dupblocks$stddev)))
predicted_author68_team6 <- M |> predicted_draws(newdata=newdata)
predicted_author68_team6 |> ggplot(aes(x=.prediction)) + stat_halfeye() + scale_x_continuous() + coord_cartesian(ylim=c(0,0.2))

```

If we instead put author 68 into "the bad team" - team 6, the picture becomes radically different. There is much more probability to add up to 25 issues (with the extremes up to 100, which probably is an exaggeration). The point is that the team 6 change caused the probabilities to radically change.

```{r}
newdata <- data.frame(t(c(file=17, author=68, team=6, ADD=300/scale_added$stddev, DUP=6/scale_dupblocks$stddev)))
predicted_author68_team6 <- M |> predicted_draws(newdata=newdata)
predicted_author68_team6 |> ggplot(aes(x=.prediction)) + stat_halfeye() + scale_x_continuous() + coord_cartesian(ylim=c(0,0.2))

```

If we instead look at the same change, from the perspective of authors that never introduced any issue:

```{r}
newdata <- data.frame(t(c(file=17, author=17, team=6, ADD=300/scale_added$stddev, DUP=6/scale_dupblocks$stddev)))
predicted_author68_team6 <- M |> predicted_draws(newdata=newdata)
predicted_author68_team6 |> ggplot(aes(x=.prediction)) + stat_halfeye() + scale_x_continuous() + coord_cartesian(ylim=c(0,0.2))

```

```{r}
newdata <- data.frame(t(c(file=17, author=17, team=2, ADD=300/scale_added$stddev, DUP=6/scale_dupblocks$stddev)))
predicted_author68_team6 <- M |> predicted_draws(newdata=newdata)
predicted_author68_team6 |> ggplot(aes(x=.prediction)) + stat_halfeye() + scale_x_continuous() + coord_cartesian(ylim=c(0,0.2))

```


So, clearly, the model thinks that putting author 68 in team 6 will make it much more likely for issues to appear!
Both the width and the height of the probabilities increase a lot with team 6, relative to the original team 7.


### Conditional Effects

Following on https://www.andrewheiss.com/blog/2022/11/29/conditional-marginal-marginaleffects/

"Fixed effects" in multilevel models are not the same as "fixed effects" in econometrics.

Marginal effects in single-level models are the change in the outcome while moving one of the explanatory variables, while holding the others constant.

But Multilevel Marginal Effects are different than these two.
In multilevel models, you calculate both *marginal effects* and *conditional effects*. Neither are necessary related to slopes, though they can be.

In a model looking like `mod <- brm(y ~ x1 + x2 + (1 | child), data=d)`, we have a dataset with child-based clusters (repeated observations of each child over time), and are fitting some parameters to the data, based on both population-level variables (`x1` and `x2`), and group-based variables (intercept per child, in this case).

* **Conditional effect** is the effect on the average child
* **Marginal effect** is the effect on *children on average*

For *Conditional Effects* we set all group-specific random offsets to 0, to find the effect for a *typical* group/subject/student/child.

For *Marginal Effects*, we are looking at the *global population-level average effect*, where group-specific differences are averaged out, or integrated out, or held constant.

```{r}
plot(conditional_effects(M, points=T, ask=F))
```


Playing around with newdata. we can let the model predict the likelihood of issues being introduced.
Change the number of added lines, or the number of existing duplicates, and we can see the probability of new issues being introduced.

Questions:
*) When I introduced a specific ZI-model, I could no longer use the beta(1,1) distribution. The zi parameter was gone - I guess the normal "b" priors are then used for those parameters, right?
*) How to model the rest of the beta binomial parameters? (e.g. shape)
*) I certainly need to read, and most likely reread Andrew's excellent blog post above, though it focused on Gaussian standard models. But I interpret the "y" axis as the probability, and the x axis (at least in the last plot) as the number of introduced issues (i.e. the quantity that we let our model predict)
*) I see that I can easily predict the behaviour for a new (previously unknown) author - I guess the model then will "start out at" the average for all the authors, and use the incoming data to update the factors for that particular author. But when I add a new team, I get an error:

`Error: Levels '12' of grouping factor 'team' cannot be found in the fitted model. Consider setting argument 'allow_new_levels' to TRUE.`

I don't know if this is really relevant or not, but it is at least interesting (and shows that we should let the teams control the intercepts ---
I guess the author slope is the amount of change that , and let the authors control the slopes - I just need to wrap my head around "what is the unit on the x-axis that the slopes vary over"? Is it like an indicator variable, which can be 0 or 1 (i.e. an adjustment for a particular author, given a particular team --- that is, an intercept).
*) What is the easiest way to integrate over the above predictions? I mean, to put a figure on the amount of probability difference between putting author 68 in team 6 or in team 7?

*) AP Anders: Read up on prior predictive checks using brms and the tidyverse!
